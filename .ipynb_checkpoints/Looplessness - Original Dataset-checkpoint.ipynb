{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recent-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "import seaborn as sns\n",
    "import random as random\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "import matplotlib.patches as mpatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "european-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "promotional-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leading_eigenvalue(adj_matrix):\n",
    "    w, v = LA.eig(adj_matrix)\n",
    "    lamd_i = max(w)\n",
    "    lamd_i = lamd_i.real\n",
    "    return lamd_i\n",
    "\n",
    "#Non normal networks\n",
    "def get_non_normal(adj_matrix):\n",
    "    A_norm = np.linalg.norm(adj_matrix)\n",
    "    w, v = LA.eig(adj_matrix)\n",
    "    sum_lambda = sum(w.real**2)\n",
    "    Df = np.sqrt(A_norm**2 - sum_lambda )\n",
    "    d_f = Df/A_norm\n",
    "    return d_f\n",
    "\n",
    "\n",
    "#lower bounder normality K = L/N\n",
    "def get_lower_bound(tau, k = 1.23):\n",
    "    dl_f = np.sqrt(1 - (1/k)*np.exp(2*tau))\n",
    "    return dl_f\n",
    "\n",
    "#upper bounder normality K = L/N\n",
    "def get_upper_bound(tau, L = 15317):\n",
    "    dl_f = np.sqrt(1 - (1/L)*np.exp(2*tau))\n",
    "    return dl_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loaded-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "##tau -- loop exponent\n",
    "def tau(alpha, qtilde, q):\n",
    "    return np.log(alpha) + 1/(2*qtilde**2) -1/(2*q**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "detected-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Lb(D):\n",
    "    \n",
    "    Lb_ensemble = []\n",
    "    basal_ensemble_len = 0\n",
    "    #in degree sequence\n",
    "    din = list(d for n, d in D.in_degree())\n",
    "\n",
    "    #out degree sequence\n",
    "    dout = list(d for n, d in D.out_degree())\n",
    "\n",
    "    while basal_ensemble_len != 20:\n",
    "        search_basal_graph=False\n",
    "        trial=0\n",
    "        while search_basal_graph==False:\n",
    "            #print(\"Trial: \", trial)\n",
    "            \n",
    "            # obtain a random graph from the degree sequences\n",
    "            D1 = nx.directed_configuration_model(din, dout )\n",
    "            D1 = nx.DiGraph(D1)\n",
    "            D1.remove_edges_from(nx.selfloop_edges(D1))\n",
    "\n",
    "            L = D1.number_of_edges()\n",
    "\n",
    "            #Get the basal nodes (in degree = 0 )\n",
    "            in_degree = list(D1.in_degree())\n",
    "            out_degree = list(D1.out_degree())\n",
    "\n",
    "            #number of basal nodes\n",
    "            B = 0\n",
    "\n",
    "            #number of basal edges\n",
    "            basal_edges = []\n",
    "            for i in range(len(in_degree)):\n",
    "                if in_degree[i][1] == 0:\n",
    "                    B += 1\n",
    "                    basal_edges.append(out_degree[i][1])\n",
    "\n",
    "            #print(\"Number of basal nodes B:\", B) ##same as the paper\n",
    "\n",
    "            if B != 0:\n",
    "                L_b = sum(basal_edges)\n",
    "            else:\n",
    "                print(\"No basal nodes present\")\n",
    "                #break\n",
    "            #print(L_b)\n",
    "\n",
    "            basal_nodes=[]\n",
    "            for node, indeg in dict(D1.in_degree()).items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "                if indeg == 0:\n",
    "                    basal_nodes.append(node)\n",
    "\n",
    "            loop_complete=0\n",
    "            for nonbasal_node, indeg in dict(D1.in_degree()).items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "                loop_complete+=1\n",
    "                if indeg != 0: # not a basal node\n",
    "                    count=0\n",
    "                    for i in basal_nodes:\n",
    "                        count+=list(D1.edges()).count((i,nonbasal_node)) # count if it is there or not (1--->2, 2 eats 1)\n",
    "                    #print(nonbasal_node,\":\",indeg*L_b/L, count)    \n",
    "                    #only if both these condition holds, then only we break the loop\n",
    "                    if math.ceil(indeg*L_b/L)!=count and math.floor(indeg*L_b/L)!=count : #soft constraints\n",
    "                        break\n",
    "            if loop_complete==len(dict(D1.in_degree())):\n",
    "                search_basal_graph=True\n",
    "                Lb_ensemble.append(L_b)\n",
    "                basal_ensemble_len+=1\n",
    "            trial+=1\n",
    "    #random.seed(12345)\n",
    "    Lb = random.choice(Lb_ensemble)\n",
    "    return Lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exciting-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Lb_dce(D):\n",
    "    Lb_ensemble = []\n",
    "    directConfig_ensemble_len = 0\n",
    "\n",
    "    while directConfig_ensemble_len != 100:\n",
    "        \n",
    "\n",
    "        #in degree sequence\n",
    "        din = list(d for n, d in D.in_degree())\n",
    "\n",
    "        #out degree sequence\n",
    "        dout = list(d for n, d in D.out_degree())\n",
    "\n",
    "        # obtain a random graph from the degree sequences\n",
    "        D1 = nx.directed_configuration_model(din, dout,seed=12345)\n",
    "        D1 = nx.DiGraph(D1)\n",
    "        D1.remove_edges_from(nx.selfloop_edges(D1))\n",
    "\n",
    "        L = D1.number_of_edges()\n",
    "\n",
    "        #Get the basal nodes (in degree = 0 )\n",
    "        in_degree = list(D1.in_degree())\n",
    "        out_degree = list(D1.out_degree())\n",
    "\n",
    "        #number of basal nodes\n",
    "        B = 0\n",
    "\n",
    "        #number of basal edges\n",
    "        basal_edges = []\n",
    "        for i in range(len(in_degree)):\n",
    "            if in_degree[i][1] == 0:\n",
    "                B += 1\n",
    "                basal_edges.append(out_degree[i][1])\n",
    "\n",
    "        #print(\"Number of basal nodes B:\", B) ##same as the paper\n",
    "\n",
    "        if B != 0:\n",
    "            L_b = sum(basal_edges)\n",
    "            directConfig_ensemble_len+=1\n",
    "            Lb_ensemble.append(L_b)\n",
    "        \n",
    "        else:\n",
    "            print(\"No basal nodes present\")\n",
    "            #break\n",
    "        #print(L_b)\n",
    "    random.seed(12345)\n",
    "    Lb=random.choice(Lb_ensemble)\n",
    "    return Lb\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "radio-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_properties(G):\n",
    "    #number of nodes\n",
    "    N = G.number_of_nodes()\n",
    "    print(\"Number of nodes N:\", N)\n",
    "    \n",
    "\n",
    "    connected_component_subgraphs = (G.subgraph(c) for c in nx.strongly_connected_components(G))\n",
    "\n",
    "    G_giant = max(connected_component_subgraphs, key=len)\n",
    "    \n",
    "\n",
    "    N_giant = G_giant.number_of_nodes()\n",
    "    \n",
    "    #number of edges\n",
    "    L = G.number_of_edges()\n",
    "\n",
    "    print(\"Number of edges L:\", L)\n",
    "    \n",
    "    #Get the basal nodes (in degree = 0 )\n",
    "    in_degree = list(G.in_degree())\n",
    "    out_degree = list(G.out_degree())\n",
    "    \n",
    "    in_degree_giant = list(G_giant.in_degree())\n",
    "\n",
    "    B_giant = 0\n",
    "    for i in range(len(in_degree_giant)):\n",
    "        if in_degree_giant[i][1] == 0:\n",
    "            B_giant += 1\n",
    "    frac_non_bassal = (N_giant - B_giant) /N\n",
    "            \n",
    "    #number of basal nodes\n",
    "    B = 0\n",
    "    \n",
    "    #number of basal edges\n",
    "    basal_edges = []\n",
    "    for i in range(len(in_degree)):\n",
    "        if in_degree[i][1] == 0:\n",
    "            B += 1\n",
    "            basal_edges.append(out_degree[i][1])\n",
    "            \n",
    "    print(\"Number of basal nodes B:\", B) ##same as the paper\n",
    "    \n",
    "    if B != 0:\n",
    "\n",
    "        #L_b\n",
    "        L_b = sum(basal_edges)\n",
    "        print(\"Number of basal edges L_B:\", L_b) ##same as the paper\n",
    "\n",
    "        #average degree\n",
    "        k = sum(d for n, d in G.in_degree()) / float(N)\n",
    "        print(\"Average in degree: %8.4f\\n\" % k)\n",
    "\n",
    "        #Compute tropich levels\n",
    "        trophic_levels = nx.algorithms.centrality.trophic_levels(G)\n",
    "        trophic_levels = [(k, v) for k, v in trophic_levels.items()] \n",
    "        s = []\n",
    "        for i in range(len(trophic_levels)):\n",
    "            s.append(trophic_levels[i][1])\n",
    "\n",
    "        \n",
    "        #trophic incoherence parameter \n",
    "        q = nx.trophic_incoherence_parameter(G)\n",
    "        q_tilde = np.sqrt( L / L_b -1)\n",
    "\n",
    "        print(\"Trophic incoherence parameter q: \", q)\n",
    "        #quotient\n",
    "        q_qt = q/q_tilde\n",
    "\n",
    "        print(\"Quotient q/q_tilde = \", q_qt)\n",
    "\n",
    "        s_tilde = 1 + (1- B/N)*(L/L_b)\n",
    "        s_st = sum(s)/len(s)*1/s_tilde\n",
    "\n",
    "\n",
    "        print(\"Quotient s/s_tilde = \", s_st)\n",
    "\n",
    "        out = dict(G.out_degree())\n",
    "        inn = dict(G.in_degree())\n",
    "        out = list(out.values())\n",
    "        inn = list(inn.values())\n",
    "\n",
    "        #compute alpha\n",
    "        kinkout =0\n",
    "        for i in range(len(out)):\n",
    "            kinkout += out[i] * inn[i]\n",
    "\n",
    "        alpha_num = kinkout / float(N)\n",
    "        alpha = alpha_num/k\n",
    "        alpha_tilde = (L - L_b) / (N - B)\n",
    "        a_at = alpha / alpha_tilde ##again correct\n",
    "\n",
    "        print(\"Quotient alpha/alpha_tilde = \", a_at)\n",
    "\n",
    "\n",
    "        #compute tau\n",
    "        tauu = tau(alpha, q_tilde, q) ##again correct\n",
    "        print(\"Loop exponent tau =\", tauu)\n",
    "\n",
    "\n",
    "        A = nx.adjacency_matrix(G)\n",
    "        adj_ = pd.DataFrame(A.todense())\n",
    "        lamda_i = get_leading_eigenvalue(adj_)\n",
    "        d_f = get_non_normal(adj_)\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #if no bassal nodes everything 0\n",
    "        N = 0\n",
    "        k =0\n",
    "        q = 0\n",
    "        q_tilde =0\n",
    "        q_qt = 0\n",
    "        s_st = 0\n",
    "        a_at = 0\n",
    "        tauu =0 \n",
    "        lamda_i = 0\n",
    "        alpha = 0\n",
    "        L_b=0\n",
    "        d_f = 0 \n",
    "        frac_non_bassal = 0\n",
    "\n",
    "    df2 = pd.DataFrame([N, B, L_b, round(k, 2), round(q, 2), round(q_tilde, 2), round(q_qt, 2), round(s_st, 2), round(a_at, 2), round(tauu, 2), round(lamda_i, 2), round(alpha, 2), round(d_f, 2), round(frac_non_bassal, 2)])\n",
    "    return df2.T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "helpful-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "# Get a list of all the dat files\n",
    "dat_files_food = glob.glob('Network_Data_MJS20/FoodWebs/*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "welcome-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(dat_files):\n",
    "    df_results = pd.DataFrame()\n",
    "    files_names = []\n",
    "    acyclic = []\n",
    "    for i in range(len(dat_files)):\n",
    "        x = np.loadtxt(dat_files[i])\n",
    "        data = pd.DataFrame(x)\n",
    "        x = list(zip(data[1], data[0]))\n",
    "        G_ = nx.DiGraph(x)\n",
    "        G_.remove_edges_from(nx.selfloop_edges(G_))\n",
    "        acy = nx.is_directed_acyclic_graph(G_)\n",
    "        acyclic.append(acy)\n",
    "        df_tmp_res = get_main_properties(G_)\n",
    "        df_results = df_results.append(df_tmp_res)\n",
    "        head, tail = os.path.split(dat_files[i])\n",
    "        files_names.append(tail[:-4])\n",
    "        print(tail[:-4])\n",
    "    df_results.index = files_names\n",
    "    df_results.columns = ['N', 'B','Lb', '<k>', '$q$', '$q\\'$', 'q/q\\'', '$s/s\\'$', 'α / α\\'', '$\\tau$', '$\\lambda_1$', '$\\\\alpha$', 'df', 'phi']\n",
    "    df_results['acy'] = acyclic\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "middle-missile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes N: 29\n",
      "Number of edges L: 196\n",
      "Number of basal nodes B: 2\n",
      "Number of basal edges L_B: 9\n",
      "Average in degree:   6.7586\n",
      "\n",
      "Trophic incoherence parameter q:  0.6892367627066949\n",
      "Quotient q/q_tilde =  0.1512059514786182\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-32fa9a1fdb51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_result_food\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdat_files_food\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-d01b24126e9c>\u001b[0m in \u001b[0;36mget_info\u001b[1;34m(dat_files)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0macy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_directed_acyclic_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0macyclic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mdf_tmp_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_main_properties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mdf_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_tmp_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mhead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdat_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-9de9dfc69918>\u001b[0m in \u001b[0;36mget_main_properties\u001b[1;34m(G)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0ms_tilde\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mL_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0ms_st\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0ms_tilde\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "df_result_food = get_info(dat_files_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set_theme(style=\"ticks\")\n",
    "#sns.pairplot(df_result_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_files_genetic = glob.glob('Network_Data_MJS20/Genetic/*.dat')\n",
    "df_result_genetic = get_info(dat_files_genetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_genetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_files_Language = glob.glob('Network_Data_MJS20/Language/*.dat')\n",
    "df_result_Language = get_info(dat_files_Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_files_Metabolic = glob.glob('Network_Data_MJS20/Metabolic/*.dat')\n",
    "df_result_Metabolic = get_info(dat_files_Metabolic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_Metabolic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_files_neural = glob.glob('Network_Data_MJS20/Neural/*.dat')\n",
    "df_result_neural = get_info(dat_files_neural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_neural= df_result_neural[(df_result_neural.T != 0).any()]\n",
    "df_result_neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#social bring problems and has 0 bassal nodes\n",
    "#dat_files_social = glob.glob('Network_Data_MJS20/Social/*.dat')\n",
    "#df_result_social = get_info(dat_files_social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_result_social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_files_Trade = glob.glob('Network_Data_MJS20/Trade/*.dat')\n",
    "df_result_Trade = get_info(dat_files_Trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_Trade= df_result_Trade[(df_result_Trade.T != 0).any()]\n",
    "df_result_Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "x = np.linspace(-35, 3.2)\n",
    "y = np.exp(x)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "sns.scatterplot(data=df_result_food, x=\"$\\tau$\", y=\"phi\", alpha=.5, s= 200,  palette=\"muted\", marker = \"v\", label = 'Food Web')\n",
    "sns.scatterplot(data=df_result_genetic, x=\"$\\tau$\", y=\"phi\", alpha=.5, s= 200,  palette=\"muted\", marker = \"D\", label = 'Genetic')\n",
    "sns.scatterplot(data=df_result_Language, x=\"$\\tau$\", y=\"phi\", alpha=.5, s= 200,  palette=\"muted\", marker = \"<\", label = 'Language')\n",
    "sns.scatterplot(data=df_result_Metabolic, x=\"$\\tau$\", y=\"phi\", alpha=.5, s= 200,  palette=\"muted\",  label = 'Metabolic')\n",
    "sns.scatterplot(data=df_result_neural, x=\"$\\tau$\", y=\"phi\", alpha=.5, s= 200,  palette=\"muted\", marker = \"s\", label = 'Neural')\n",
    "sns.scatterplot(data=df_result_Trade, x=\"$\\tau$\", y=\"phi\", alpha=.5, s= 200,  palette=\"muted\", marker = \"*\", label = 'Trade')\n",
    "#plt.plot(x, y, label = '$\\\\bar{\\\\lambda}_i$')\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.xlabel('$\\\\tau$')\n",
    "plt.ylabel('$\\phi$')\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.25, 0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "x = np.linspace(-35, 3.2)\n",
    "y = np.exp(x)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "sns.scatterplot(data=df_result_food, x=\"$\\tau$\", y=\"$\\\\lambda_1$\", alpha=.5, s= 200,  palette=\"muted\", marker = \"v\", label = 'Food Web')\n",
    "sns.scatterplot(data=df_result_genetic, x=\"$\\tau$\", y=\"$\\\\lambda_1$\", alpha=.5, s= 200,  palette=\"muted\", marker = \"D\", label = 'Genetic')\n",
    "sns.scatterplot(data=df_result_Language, x=\"$\\tau$\", y=\"$\\\\lambda_1$\", alpha=.5, s= 200,  palette=\"muted\", marker = \"<\", label = 'Language')\n",
    "sns.scatterplot(data=df_result_Metabolic, x=\"$\\tau$\", y=\"$\\\\lambda_1$\", alpha=.5, s= 200,  palette=\"muted\",  label = 'Metabolic')\n",
    "sns.scatterplot(data=df_result_neural, x=\"$\\tau$\", y=\"$\\\\lambda_1$\", alpha=.5, s= 200,  palette=\"muted\", marker = \"s\", label = 'Neural')\n",
    "sns.scatterplot(data=df_result_Trade, x=\"$\\tau$\", y=\"$\\\\lambda_1$\", alpha=.5, s= 200,  palette=\"muted\", marker = \"*\", label = 'Trade')\n",
    "plt.plot(x, y, label = '$\\\\bar{\\\\lambda}_i$')\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.xlabel('$\\\\tau$')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.25, 0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "x = np.linspace(0, 3)\n",
    "y = np.exp(x)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "sns.scatterplot(data=df_result_food, x=\"$\\tau$\", y=\"$\\\\lambda_1$\", alpha=.5, s= 200,  palette=\"muted\", marker = \"v\", label = 'Food Web')\n",
    "sns.scatterplot(data=df_result_genetic, x=\"$\\tau$\", y=\"$\\\\lambda_1$\", alpha=.5, s= 200,  palette=\"muted\", marker = \"D\", label = 'Genetic')\n",
    "sns.scatterplot(data=df_result_Language, x=\"$\\tau$\", y=\"$\\\\lambda_1$\", alpha=.5, s= 200,  palette=\"muted\", marker = \"<\", label = 'Language')\n",
    "sns.scatterplot(data=df_result_Metabolic, x=\"$\\tau$\", y=\"$\\\\lambda_1$\", alpha=.5, s= 200,  palette=\"muted\",  label = 'Metabolic')\n",
    "sns.scatterplot(data=df_result_neural, x=\"$\\tau$\", y=\"$\\\\lambda_1$\", alpha=.5, s= 200,  palette=\"muted\", marker = \"s\", label = 'Neural')\n",
    "sns.scatterplot(data=df_result_Trade, x=\"$\\tau$\", y=\"$\\\\lambda_1$\", alpha=.5, s= 200,  palette=\"muted\", marker = \"*\", label = 'Trade')\n",
    "plt.plot(x, y, label = '$\\\\bar{\\\\lambda}_i$')\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.xlabel('$\\\\tau$')\n",
    "plt.ylabel('$ln(\\\\lambda_i)$')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim(0,3)\n",
    "plt.ylim(1,25)\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.25, 0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,20))\n",
    "x = np.linspace(-40, 10)\n",
    "y = get_lower_bound(x)\n",
    "y2 = get_upper_bound(x)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "sns.scatterplot(data=df_result_food, x=\"$\\tau$\", y=\"df\", alpha=.5, s= 200,  palette=\"muted\", marker = \"v\", label = 'Food Web')\n",
    "sns.scatterplot(data=df_result_genetic, x=\"$\\tau$\", y=\"df\", alpha=.5, s= 200,  palette=\"muted\", marker = \"D\", label = 'Genetic')\n",
    "sns.scatterplot(data=df_result_Language, x=\"$\\tau$\", y=\"df\", alpha=.5, s= 200,  palette=\"muted\", marker = \"<\", label = 'Language')\n",
    "sns.scatterplot(data=df_result_Metabolic, x=\"$\\tau$\", y=\"df\", alpha=.5, s= 200,  palette=\"muted\",  label = 'Metabolic')\n",
    "sns.scatterplot(data=df_result_neural, x=\"$\\tau$\", y=\"df\", alpha=.5, s= 200,  palette=\"muted\", marker = \"s\", label = 'Neural')\n",
    "sns.scatterplot(data=df_result_Trade, x=\"$\\tau$\", y=\"df\", alpha=.5, s= 200,  palette=\"muted\", marker = \"*\", label = 'Trade')\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.xlabel('$\\\\tau$')\n",
    "plt.ylabel('$d_F$')\n",
    "\n",
    "plt.xlim(-40,10)\n",
    "plt.ylim(0.4,1.1)\n",
    "\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-40, 10)\n",
    "y_2 = get_lower_bound(x, 2)\n",
    "y_3 = get_lower_bound(x, 3)\n",
    "y_10 = get_lower_bound(x, 10)\n",
    "y_20 = get_lower_bound(x, 20)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "plt.plot(x, y_2, label  = 'k = 2')\n",
    "plt.plot(x, y_3, label  = 'k = 3')\n",
    "plt.plot(x, y_10, label  = 'k = 10')\n",
    "plt.plot(x, y_20, label  = 'k = 20')\n",
    "\n",
    "plt.ylim(0.4, 1.1)\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_plot(q, qt):\n",
    "    return 1/(np.array(q)**2) -  1/(np.array(qt)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_food[\"y\"] = y_plot(df_result_food['$q$'],df_result_food['$q\\'$'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_result_genetic[\"y\"] = y_plot(df_result_genetic['$q$'],df_result_genetic['$q\\'$'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_Language[\"y\"] = y_plot(df_result_Language['$q$'],df_result_Language['$q\\'$'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_Metabolic[\"y\"] = y_plot(df_result_Metabolic['$q$'],df_result_Metabolic['$q\\'$'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_neural[\"y\"] = y_plot(df_result_neural['$q$'],df_result_neural['$q\\'$'] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_Trade[\"y\"] = y_plot(df_result_Trade['$q$'],df_result_Trade['$q\\'$'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpos=[]\n",
    "ypos=[]\n",
    "xneg=[]\n",
    "yneg=[]\n",
    "\n",
    "for df in [df_result_food,df_result_genetic,df_result_Language,df_result_Metabolic,df_result_neural,df_result_Trade]:\n",
    "    for i in range(len(df['$\\tau$'])):\n",
    "        if df['$\\tau$'][i]>0:\n",
    "            xpos.append(df[\"$\\\\alpha$\"][i])\n",
    "            ypos.append(df[\"y\"][i])\n",
    "        else:\n",
    "            xneg.append(df[\"$\\\\alpha$\"][i])\n",
    "            yneg.append(df[\"y\"][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.scatter(np.log(xpos), ypos, marker=\"^\",c='brown',label=\"$\\\\tau>0$\")\n",
    "plt.scatter(np.log(xneg), yneg, marker=\"o\",c='blue',label=\"$\\\\tau<0$\")\n",
    "plt.xlim([-1.5,4])\n",
    "plt.ylim([-20,80])\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('$\\\\frac{1}{q^2} - \\\\frac{1}{q\\'^2}$')\n",
    "plt.xlabel('$ln(\\\\alpha)$')\n",
    "\n",
    "# plt tau = 0 line\n",
    "plt.plot(np.linspace(-1.5,4,100),np.linspace(-1.5,4,100),dashes=[1,1],\n",
    "           markersize=1,c='grey')\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-bride",
   "metadata": {},
   "source": [
    "## Average properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_q(df, quantitie):\n",
    "    mean = round(np.mean(df[quantitie]), 2)\n",
    "    std = round(np.std(df[quantitie]), 2)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = [df_result_food,  df_result_genetic, df_result_Language, df_result_Metabolic, df_result_neural, df_result_Trade]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.DataFrame()\n",
    "for j  in (df_result_food.columns[6:11]):\n",
    "    val = []\n",
    "    for i in range(len(df_all)):\n",
    "        mean, std = get_average_q(df_all[i], j)\n",
    "        val.append(str(mean)+\" $\\\\pm$ \"+str(std))\n",
    "    datos[j] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.index = ['Food', 'Genetic', 'Language', 'Metabolic', 'Neural', 'Trade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tau = []\n",
    "n_acy = []\n",
    "for i in range(len(df_all)):\n",
    "    count = 0\n",
    "    count_acy =0\n",
    "    values = np.sign(df_all[i]['$\\tau$'])\n",
    "    for j in range(len(values)):\n",
    "        if (values[j] < 0):\n",
    "            count += 1\n",
    "        if df_all[i]['acy'][j] == True:\n",
    "            count_acy +=1\n",
    "        \n",
    "    negative_tau.append(str(count)+'/'+str(len(df_all[i])))\n",
    "    n_acy.append(str(count_acy)+'/'+str(len(df_all[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['  $\\\\tau $ < 0   '] = negative_tau\n",
    "datos['Acyclic'] = n_acy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prob acyclic\n",
    "def p_acy(df):\n",
    "    results = []\n",
    "    for i in range(len(df)):\n",
    "        a1 = 1/(df['α / α\\''][i])\n",
    "        a2 = 1/(df['q/q\\''][i])\n",
    "        tau = df['$\\tau$'][i]\n",
    "        val = np.exp(-a1 * a2 * (1/(np.exp(-tau)-1)))\n",
    "        results.append(val)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_acyclic(df_):\n",
    "    p_acyclic = p_acy(df_)\n",
    "    df_['P'] = p_acyclic \n",
    "    n_acycl = len(df_[df_['P'] > 0.5])\n",
    "    return n_acycl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "acyclyc = []\n",
    "for i in range(len(df_all)):\n",
    "    n_acycl = count_acyclic(df_all[i])\n",
    "    acyclyc.append(n_acycl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "acyclyc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-worship",
   "metadata": {},
   "source": [
    "# Applying perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_pos = pd.DataFrame({'x1':np.log(xpos),'x2':ypos,'y':[1]*len(xpos)})\n",
    "df_neg = pd.DataFrame({'x1':np.log(xneg),'x2':yneg,'y':[-1]*len(xneg)})\n",
    "\n",
    "df_perp = pd.concat([df_pos,df_neg],ignore_index=True)\n",
    "df_perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataframe\n",
    "df_perp = df_perp.sample(frac=1).reset_index(drop=True)\n",
    "df_perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = df_perp.iloc[:int(0.85*len(df_perp)),:]\n",
    "testdata = df_perp.iloc[int(0.85*len(df_perp)):,:]\n",
    "\n",
    "train_x=traindata.iloc[:,:-1]\n",
    "train_y=traindata.iloc[:,-1]\n",
    "\n",
    "test_x=testdata.iloc[:,:-1]\n",
    "test_y=testdata.iloc[:,-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce the perceptron \n",
    "MaxIter=30\n",
    "per=Perceptron(max_iter=MaxIter, eta0=0.1,shuffle=True)\n",
    "per.fit(train_x, train_y)\n",
    "Train_y = pd.Series(per.predict(train_x), name='y') \n",
    "Test_y=pd.Series(per.predict(test_x), name='y')\n",
    "testdata=test_x.join(Test_y, how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "\n",
    "#plot the train data set\n",
    "label=train_y.copy()\n",
    "label[label<0]=0\n",
    "label=label.astype(int)\n",
    "label=label.values\n",
    "colormap=np.array(['r','b'])\n",
    "plt.scatter(train_x.iloc[:,0], train_x.iloc[:,1], marker='o', c=colormap[label])\n",
    "\n",
    "#plot the test data set\n",
    "labelt=Test_y.copy()\n",
    "labelt[labelt<0]=0\n",
    "labelt=labelt.astype(int)\n",
    "labelt=labelt.values\n",
    "plt.scatter(test_x.iloc[:,0], test_x.iloc[:,1], marker='+', c=colormap[labelt])\n",
    "\n",
    "#calculate the hyperplane\n",
    "w=per.coef_[0]\n",
    "xx=np.linspace(-1.5, 4)\n",
    "yy=-(w[0]*xx+per.intercept_[0])/w[1]\n",
    "\n",
    "#plot the line\n",
    "hyperplane, = plt.plot(xx, yy, dashes=[3,1],markersize=1,c='black', label='$hyperplane$')\n",
    "#plt.title(u'Iteration= %d' % MaxIter)\n",
    "\n",
    "\n",
    "# plt tau = 0 line\n",
    "tau_zero, = plt.plot(np.linspace(-1.5,4,100),np.linspace(-1.5,4,100),dashes=[1,1],\n",
    "           markersize=1,c='green',label=\"$\\\\tau=0$ line\")\n",
    "\n",
    "train_marker = plt.scatter([],[],c='grey',marker='o',label='train') \n",
    "test_marker = plt.scatter([],[],c='grey',marker='+',label='test')\n",
    "\n",
    "tau_neg=mpatches.Patch(color='red', label=\"$\\\\tau<0$\")\n",
    "tau_pos=mpatches.Patch(color='blue', label=\"$\\\\tau>0$\")\n",
    "\n",
    "plt.xlim([-1.5,4])\n",
    "plt.ylim([-20,80])\n",
    "#plt.xticks(np.arange(-1.5,4,0.5))\n",
    "#plt.yticks(range(-20,80,20))\n",
    "\n",
    "plt.tick_params(direction='in',bottom=True,top=True,left=True,right=True)\n",
    "plt.grid(linestyle='--',alpha=0.8,which='both')\n",
    "\n",
    "plt.ylabel('$\\\\frac{1}{q^2} - \\\\frac{1}{q\\'^2}$')\n",
    "plt.xlabel('$ln(\\\\alpha)$')\n",
    "\n",
    "plt.legend(loc='best',handles=[hyperplane, tau_zero, tau_neg,tau_pos, train_marker,test_marker])\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig(path+’¥¥perceptron.png’)plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the accuracy rate for inseparable data sets\n",
    "count=0\n",
    "for i in range(len(Test_y)):\n",
    "    if test_y.iloc[i]==Test_y.iloc[i]:\n",
    "        count+=1.0\n",
    "accuracy=count/float(len(Test_y))*100\n",
    "print ('Test Accuracy rate: %.2f%%' % accuracy)\n",
    "\n",
    "count=0\n",
    "for i in range(len(Train_y)):\n",
    "    if train_y.iloc[i]==Train_y.iloc[i]:\n",
    "        count+=1.0\n",
    "accuracy=count/float(len(Train_y))*100\n",
    "print ('Train Accuracy rate: %.2f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-pattern",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "administrative-resort",
   "metadata": {},
   "source": [
    "# P_acyclic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L_Lb = 10\n",
    "#alpha = 1\n",
    "\n",
    "def P_acyclic(L_Lb, alpha):\n",
    "    q_tilde = np.sqrt(L_Lb-1)\n",
    "\n",
    "\n",
    "    q = np.arange(q_tilde,0.15,-0.01)\n",
    "    tau = np.log(alpha) + 0.5*(1/(q_tilde**2) - 1/(q**2) ) \n",
    "    \n",
    "    # dempsters example, check other examples\n",
    "    #L=3169\n",
    "    #N=1624\n",
    "    #B=1542\n",
    "    \n",
    "    #coweeta example\n",
    "    L=148\n",
    "    N=71\n",
    "    B=38\n",
    "    \n",
    "    Lb = L/L_Lb\n",
    "    \n",
    "    alpha_tilde = (L-Lb)/(N-B)#L_Lb-1#\n",
    "    print(alpha,alpha_tilde)\n",
    "    P_acy = np.exp( -(alpha_tilde/alpha)*(q_tilde/q)*(np.exp(-tau) - 1)**-1  )\n",
    "    x = q**-2 - q_tilde**-2\n",
    "    return(x, P_acy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(P_acyclic(10, 1)[0],P_acyclic(10, 1)[1],c='brown',label='L/Lb=10, $\\\\alpha=1$')\n",
    "\n",
    "plt.plot(P_acyclic(100, 1)[0],P_acyclic(100, 1)[1],'--',c='blue',label='L/Lb=100, $\\\\alpha=1$')\n",
    "#plt.plot(P_acyclic(100, 2)[0],P_acyclic(100, 2)[1],c='green')\n",
    "plt.gca().invert_xaxis()\n",
    "plt.xlim([25,1])\n",
    "plt.grid(False)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('$\\\\frac{1}{q^2} - \\\\frac{1}{q\\'^2}$')\n",
    "plt.ylabel('P')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-plumbing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-norfolk",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-johns",
   "metadata": {},
   "source": [
    "## Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-breast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-jefferson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-blocking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-education",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-iraqi",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
